{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kogpt2_chatbot_train.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "eIYN6HpnbCQ5",
        "5vBsRwku3XkP"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sA8ImJNJcexD"
      },
      "source": [
        "# 1. Google drive 연동\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3alNmQLdXAhE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8a68d4f-65c7-4716-a316-d16faa66794f"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "# google mount\n",
        "drive.mount('/content/gdrive')\n",
        "os.chdir('./gdrive/My Drive/chatbot/KoGPT2')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U1nZnTnwPBs"
      },
      "source": [
        "# 2. GPU 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUJmLHWayamd",
        "outputId": "ebbc889f-4bc5-454c-cb5d-4e5c3f133a7a"
      },
      "source": [
        "import torch\r\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nwJ1vnWwSLV",
        "outputId": "8549f74a-5793-4fbb-e635-577ec2bbeaaf"
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Dec  9 23:10:13 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    26W / 250W |     10MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEfafZvScpgq"
      },
      "source": [
        "# 2. 1 필요 패키지 설치"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PIkNjcO5kXDe",
        "outputId": "cb94084c-61a4-43bc-a2e0-d1d755811dfa"
      },
      "source": [
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/My Drive/chatbot/KoGPT2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H3KHT2gkgTA"
      },
      "source": [
        "! git clone https://github.com/SKT-AI/KoGPT2.git\r\n",
        "cd KoGPT2\r\n",
        "! pip install -r requirements.txt\r\n",
        "! pip install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XW2zqtVczZm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6b97af12-7c56-471f-b172-e049c8709f20"
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import torchtext\n",
        "import time\n",
        "import datetime\n",
        "from tqdm import tqdm\n",
        "from kogpt2.pytorch_kogpt2 import get_pytorch_kogpt2_model\n",
        "from gluonnlp.data import SentencepieceTokenizer\n",
        "from kogpt2.utils import get_tokenizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gluonnlp==0.9.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/27/07b57d22496ed6c98b247e578712122402487f5c265ec70a747900f97060/gluonnlp-0.9.1.tar.gz (252kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 5.4MB/s \n",
            "\u001b[?25hCollecting mxnet==1.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/f5/d79b5b40735086ff1100c680703e0f3efc830fa455e268e9e96f3c857e93/mxnet-1.6.0-py2.py3-none-any.whl (68.7MB)\n",
            "\u001b[K     |████████████████████████████████| 68.7MB 94kB/s \n",
            "\u001b[?25hCollecting sentencepiece>=0.1.85\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 44.6MB/s \n",
            "\u001b[?25hCollecting torch==1.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/70/54e9fb010fe1547bc4774716f11ececb81ae5b306c05f090f4461ee13205/torch-1.5.0-cp36-cp36m-manylinux1_x86_64.whl (752.0MB)\n",
            "\u001b[K     |████████████████████████████████| 752.0MB 23kB/s \n",
            "\u001b[?25hCollecting transformers==2.11.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 52.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from gluonnlp==0.9.1->-r requirements.txt (line 1)) (1.19.4)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from gluonnlp==0.9.1->-r requirements.txt (line 1)) (0.29.21)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from gluonnlp==0.9.1->-r requirements.txt (line 1)) (20.8)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet==1.6.0->-r requirements.txt (line 2)) (2.23.0)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.0->-r requirements.txt (line 4)) (0.16.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0->-r requirements.txt (line 5)) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0->-r requirements.txt (line 5)) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 47.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0->-r requirements.txt (line 5)) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0->-r requirements.txt (line 5)) (0.8)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 48.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp==0.9.1->-r requirements.txt (line 1)) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0->-r requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0->-r requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0->-r requirements.txt (line 2)) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0->-r requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.11.0->-r requirements.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.11.0->-r requirements.txt (line 5)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.11.0->-r requirements.txt (line 5)) (1.0.0)\n",
            "Building wheels for collected packages: gluonnlp, sacremoses\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.9.1-cp36-cp36m-linux_x86_64.whl size=470040 sha256=8e601d7a840a58faa598671b3b592bc81d002fe4c726c46cde9d461ad3f26ad2\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/60/16/1f8a40e68b85bd9bd7960e91830bca5e40cd113f3220b7e231\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=feb76483190cffe7932c14193282f8d658e1d6febb6fee4e203ba8ba22cbf2be\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built gluonnlp sacremoses\n",
            "\u001b[31mERROR: torchvision 0.8.1+cu101 has requirement torch==1.7.0, but you'll have torch 1.5.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gluonnlp, graphviz, mxnet, sentencepiece, torch, sacremoses, tokenizers, transformers\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "  Found existing installation: torch 1.7.0+cu101\n",
            "    Uninstalling torch-1.7.0+cu101:\n",
            "      Successfully uninstalled torch-1.7.0+cu101\n",
            "Successfully installed gluonnlp-0.9.1 graphviz-0.8.4 mxnet-1.6.0 sacremoses-0.0.43 sentencepiece-0.1.94 tokenizers-0.7.0 torch-1.5.0 transformers-2.11.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIYN6HpnbCQ5"
      },
      "source": [
        "# 3. Data 불러오기\n",
        "1. Chatbot_data\t일상 챗봇 학습용 문답 페어 11,876건\n",
        "일상(0) / 이별,부정(1) / 사랑,긍정(2) 라벨 부여 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvPtmGnItyAZ"
      },
      "source": [
        "# 3.1 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq96mYhhzjf6"
      },
      "source": [
        "os.chdir('..')\r\n",
        "tok_path = get_tokenizer()\r\n",
        "model, vocab = get_pytorch_kogpt2_model()\r\n",
        "tokenizer = SentencepieceTokenizer(tok_path,  num_best=0, alpha=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gXGJTPpyiTZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbf3437d-f15c-473f-ad90-cc00a871f089"
      },
      "source": [
        "tok_path = get_tokenizer()\r\n",
        "# model, vocab\r\n",
        "model, vocab = get_pytorch_kogpt2_model()\r\n",
        "# tokenizer\r\n",
        "tokenizer = SentencepieceTokenizer(tok_path,  num_best=0, alpha=0)\r\n",
        "# bos token, eos token 붙이기\r\n",
        "class make_data_set:\r\n",
        "    def __init__(self):\r\n",
        "        '''\r\n",
        "        data : data frame\r\n",
        "        data - Q, A 로 구성 \r\n",
        "        '''\r\n",
        "        data=pd.read_csv('https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData%20.csv',sep=',',header=0)\r\n",
        "        # <usr> sentence <cls>\r\n",
        "        data['Q_input_id']=data['Q'].apply(lambda i :[vocab['<usr>']]+vocab[tokenizer(i)]+[vocab['<unused0>']])\r\n",
        "        # <sys> sentence </s>\r\n",
        "        data['A_input_id']=data['A'].apply(lambda i :[vocab['<sys>']]+vocab[tokenizer(i)]+[vocab['</s>']])\r\n",
        "        data['cls_position']=None\r\n",
        "        Max = config.max_len\r\n",
        "        for i in data.index:\r\n",
        "            # Q+A의 길이가 Max보다 길면 Q은 -Max//2부터 끝까지.\r\n",
        "            if len(data.Q_input_id[i])+len(data.A_input_id[i])>Max:\r\n",
        "                data.Q_input_id[i]=data.Q_input_id[i][-(Max//2):]\r\n",
        "                data.A_input_id[i]=data.A_input_id[i][:(Max-(Max//2))]\r\n",
        "            # data['attention_mask'][i] = [1] * (len(data.Q_input_id[i])+len(data.A_input_id[i]))+[0] * (Max - (len(data.Q_input_id[i])+len(data.A_input_id[i])))\r\n",
        "            # data['token_id'][i] = [0] * len(data.Q_input_id[i]) + [1] * (Max-len(data.Q_input_id[i]))\r\n",
        "            # classification을 위해서\r\n",
        "            data['cls_position'][i] = len(data.Q_input_id[i])-1 \r\n",
        "        data['input_id']=data['Q_input_id']+data['A_input_id']\r\n",
        "        # pad 하기\r\n",
        "        data['input_id']=data['input_id'].apply(lambda j : j+(Max-len(j))*[vocab['<pad>']])\r\n",
        "        # <usr> sentence <cls> | <sys> sentence  | <eos> pad ...\r\n",
        "        # pad, pad , ... pad,  | sentence, <eos> | pad, pad ...\r\n",
        "\r\n",
        "        data['lm_label']=data['Q_input_id'].apply(lambda i : len(i) * [vocab['<pad>']])+data['A_input_id'].apply(lambda i : i[1:])\r\n",
        "        data['lm_label']=data['lm_label'].apply(lambda j : j+(Max-len(j))*[vocab['<pad>']])\r\n",
        "        self.data=data\r\n",
        "\r\n",
        "    def return_data_set(self):\r\n",
        "        result=self.data.loc[:,['input_id','lm_label','cls_position','label']]#'attention_mask','token_id','cls_position','lm_label','label']]\r\n",
        "        return result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[██████████████████████████████████████████████████]\n",
            "[██████████████████████████████████████████████████]\n",
            "using cached model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6qpdIrPOiVR"
      },
      "source": [
        "# 3.2 model load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1k1J31h0SBTr"
      },
      "source": [
        "class Config(dict):\n",
        "    __setattr__=dict.__setitem__\n",
        "    __getattr__=dict.__getitem__\n",
        "config=Config({'cls':3,'unk_idx':vocab[vocab.unknown_token],'bos_idx':vocab[vocab.bos_token],'eos_idx':vocab[vocab.eos_token],'cls_idx':vocab['<unused0>'],'usr_idx':vocab['<usr>'],'sys_idx':vocab['<sys>'],'padding_idx':vocab[vocab.padding_token],'n_vocab':len(vocab), 'batch_size':2, 'max_len':64})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eaSLUVfwGo9"
      },
      "source": [
        "config=Config({'cls':3,'unk_idx':vocab[vocab.unknown_token],'bos_idx':vocab[vocab.bos_token],'eos_idx':vocab[vocab.eos_token],'cls_idx':vocab['<unused0>'],'usr_idx':vocab['<usr>'],'sys_idx':vocab['<sys>'],'padding_idx':vocab[vocab.padding_token],'n_vocab':len(vocab), 'batch_size':2, 'max_len':64})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnlL4wqAcAOS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09878e23-f0b7-408d-c98e-3d89a91ba460"
      },
      "source": [
        "make_data=make_data_set(config)\n",
        "dataset=make_data.return_data_set()\n",
        "print(dataset.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                                            input_id  ... label\n",
            "0  [2, 385, 47460, 47437, 49108, 47812, 6, 4, 332...  ...     0\n",
            "1  [2, 106, 47445, 47766, 1084, 1024, 47816, 4748...  ...     0\n",
            "2  [2, 141, 47650, 47514, 47471, 2211, 47593, 299...  ...     0\n",
            "3  [2, 141, 47650, 47514, 47471, 1057, 2211, 4759...  ...     0\n",
            "4  [2, 753, 9003, 415, 20572, 6, 4, 637, 10855, 7...  ...     0\n",
            "\n",
            "[5 rows x 4 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4o7Sj5f8CKgC"
      },
      "source": [
        "### 데이터 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dyCwzTtBiXZ"
      },
      "source": [
        "dataset.to_csv('./chatbot_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1pcOz42CQ0l"
      },
      "source": [
        "### train test split\n",
        "test는 0.2 비율"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO94XbDACYCo"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train,test=train_test_split(dataset,test_size=0.2)\n",
        "val,test=train_test_split(test, test_size = 0.4)\n",
        "# 얘네도 저장\n",
        "train.to_pickle('./train_chatbot')\n",
        "test.to_pickle('./test_chatbot')\n",
        "val.to_pickle('./val_chatbot')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDHSQ8J7DgFX"
      },
      "source": [
        "# 불러오기\r\n",
        "train = pd.read_pickle('./train_chatbot')\r\n",
        "val = pd.read_pickle('./val_chatbot')\r\n",
        "test = pd.read_pickle('./test_chatbot')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wh3rbiE2CqSN"
      },
      "source": [
        "### DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppyuZNbVDJKh"
      },
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "train_dataset=TensorDataset(torch.LongTensor(train.input_id.values.tolist()),torch.LongTensor(train.cls_position.values.tolist()),torch.LongTensor(train.label.values.tolist()),torch.LongTensor(train.lm_label.values.tolist()))\n",
        "val_dataset=TensorDataset(torch.LongTensor(val.input_id.values.tolist()),torch.LongTensor(val.cls_position.values.tolist()),torch.LongTensor(val.label.values.tolist()),torch.LongTensor(val.lm_label.values.tolist()))\n",
        "test_dataset=TensorDataset(torch.LongTensor(test.input_id.values.tolist()),torch.LongTensor(test.cls_position.values.tolist()),torch.LongTensor(test.label.values.tolist()),torch.LongTensor(test.lm_label.values.tolist()))\n",
        "# train loader, test loader\n",
        "train_loader=DataLoader(train_dataset,batch_size=config.batch_size,drop_last=True)\n",
        "val_loader=DataLoader(val_dataset,batch_size=config.batch_size,drop_last=True)\n",
        "test_loader=DataLoader(test_dataset,batch_size=config.batch_size,drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSZbYfMWW33s"
      },
      "source": [
        "# 3.3 Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpXCqB76jo6I"
      },
      "source": [
        "class my_kogpt2_model(nn.Module):\n",
        "    def __init__(self, config, model):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.model = model\n",
        "        self.classification = nn.Linear(self.config.n_vocab, self.config.cls)\n",
        "\n",
        "    def inference(self,\n",
        "               input_ids,\n",
        "               ):\n",
        "        pred=self.model.generate(input_ids=input_ids,eos_token_id=vocab['</s>'],bos_token_id = vocab['<usr>'], pad_token_id = config.padding_idx, decoder_start_token_id=vocab['<sys>'],do_sample=True, max_length=64, top_p=0.92, top_k=50, temperature=0.6, no_repeat_ngram_size=None, num_return_sequences=1, early_stopping=False)\n",
        "        # input ids : 1, seq_len\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            cls_position = input_ids.squeeze(0).tolist().index(config.cls_idx)\n",
        "            output = self.model.forward(input_ids=input_ids)[0]\n",
        "            cls_output = self.classification.forward(output[:,cls_position,:])\n",
        "        return pred.squeeze().tolist(),cls_output.argmax(-1).item()\n",
        "\n",
        "    def forward(self,data):\n",
        "        input_ids = data[0]\n",
        "        cls_position = data[1]\n",
        "        #lm_label = data[-1]\n",
        "        # batch size, seq len, n_vocab\n",
        "        output = self.model.forward(input_ids=input_ids)[0]\n",
        "        e=torch.cat([output[_,j,:].unsqueeze(0) for _,j in enumerate(cls_position.tolist())],0)\n",
        "        cls_output = self.classification.forward(e)\n",
        "        return output, cls_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vBsRwku3XkP"
      },
      "source": [
        "# 4. 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLAKNPTiR3_9"
      },
      "source": [
        "import time,datetime\n",
        "def Eplased(dt):\n",
        "    d=int(round(dt))\n",
        "    return str(datetime.timedelta(seconds=d))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWh3bzl5EIkn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "556052a1-a15b-4830-c366-542c5af37036"
      },
      "source": [
        "device='cuda'\n",
        "print(device)\n",
        "epochs = 100         # Num of Epoch\n",
        "learning_rate = 1e-5\n",
        "Model = my_kogpt2_model(config, model)\n",
        "Model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate) \n",
        "criterion1 = nn.CrossEntropyLoss(ignore_index=config.padding_idx,reduction = 'sum') # language modeling\n",
        "criterion2 = nn.CrossEntropyLoss() # classification"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCSzmCe5F0qC"
      },
      "source": [
        "start_time = time.time()\n",
        "min_value = None\n",
        "for epoch in tqdm(range(1,epochs+1),desc='epoch',mininterval=600):\n",
        "    print('\\n')\n",
        "    print(epoch)\n",
        "    with torch.no_grad():\n",
        "        val_total_loss=0.\n",
        "        val_loss_lm = 0.\n",
        "        val_loss_cls = 0.\n",
        "        for data in val_loader:\n",
        "            model.eval()\n",
        "            data = tuple(i.to(device) for i in data)\n",
        "            lm_output, cls_output = Model.forward(data)        \n",
        "            loss1 = criterion1(lm_output.transpose(1,2),data[-1])\n",
        "            loss2 = criterion2(cls_output,data[-2])\n",
        "            loss = (loss1/config.batch_size+loss2)\n",
        "            val_loss_lm += (loss1/config.batch_size).item()\n",
        "            val_loss_cls += loss2.item()\n",
        "            val_total_loss+=loss.item()\n",
        "            \n",
        "        if min_value is None:\n",
        "            min_value = val_total_loss/len(val_loader) \n",
        "        if min_value < val_total_loss/len(val_loader):\n",
        "            print('early_stop')\n",
        "            print(epoch)\n",
        "            torch.save(Model.state_dict(),'./chatbot1_epoch_%d'%epoch)\n",
        "            break\n",
        "        else:\n",
        "            min_value = val_total_loss/len(val_loader)\n",
        "    total_loss=0.\n",
        "    loss_lm = 0.\n",
        "    loss_cls = 0.\n",
        "    for data in tqdm(train_loader,desc='step',mininterval=60):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        data = tuple(i.to(device) for i in data)\n",
        "        lm_output, cls_output = Model.forward(data)        \n",
        "        loss1 = criterion1(lm_output.transpose(1,2),data[-1])\n",
        "        loss2 = criterion2(cls_output,data[-2])\n",
        "        loss = (loss1/config.batch_size+loss2)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_lm += (loss1/config.batch_size).item()\n",
        "        loss_cls += loss2.item()\n",
        "        total_loss += loss.item()\n",
        "    if epoch%5==0:\n",
        "        print('epoch : %d'%epoch)\n",
        "        print('total loss : %.3f'%(total_loss/len(train_loader)))\n",
        "        print('lm loss : %.3f'%(loss_lm/len(train_loader)))\n",
        "        print('cls loss : %.3f'%(loss_cls/len(train_loader)))\n",
        "        \n",
        "        print('val total loss  : %.3f'%(val_total_loss/len(val_loader)))\n",
        "        print('val lm loss : %.3f'%(val_loss_lm/len(val_loader)))\n",
        "        print('val cls loss : %.3f'%(val_loss_cls/len(val_loader)))\n",
        "        print('Eplased Time : %s'%(Eplased(time.time()-start_time)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K15MTo-r77ET",
        "outputId": "c351b5d4-2a81-46d5-ca05-9e9b27fbed2c"
      },
      "source": [
        "import smtplib\r\n",
        "from email.mime.text import MIMEText\r\n",
        "s = smtplib.SMTP('smtp.gmail.com',587)\r\n",
        "s.starttls()\r\n",
        "s.login('tillcry8@gmail.com','xbcohxszwogqcrfj')\r\n",
        "msg = MIMEText('complete')\r\n",
        "msg['Subject'] = 'chatbot'\r\n",
        "s.sendmail(\"tillcry8@gmail.com\", \"tillcry8@gmail.com\", msg.as_string())\r\n",
        "s.quit()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(221, b'2.0.0 closing connection u9sm3476235edd.54 - gsmtp')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8B3EINROjBS"
      },
      "source": [
        "# Test data로 추정해보자"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XsTzAl5DpxP"
      },
      "source": [
        "## Model load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCDe63PXGRh8",
        "outputId": "621e7027-2986-4a59-d1eb-f7a396d8a502"
      },
      "source": [
        "! ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "chatbot1_epoch_13  T0_data\tT5_tweeter.csv\ttrain\t       val\n",
            "chatbot_data.csv   T5_tweeter\ttest\t\ttrain_chatbot  val_chatbot\n",
            "KoGPT2\t\t   T5_tweeter_\ttest_chatbot\ttweeter.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCZ8Mh0fDl9G",
        "outputId": "c3ee1286-895a-4b5b-dc3e-70f3d57d7bd7"
      },
      "source": [
        "device='cuda'\r\n",
        "Model = my_kogpt2_model(config, model)\r\n",
        "Model.to(device)\r\n",
        "criterion1 = nn.CrossEntropyLoss(ignore_index=config.padding_idx,reduction = 'sum') # language modeling\r\n",
        "criterion2 = nn.CrossEntropyLoss() # classification\r\n",
        "Model.load_state_dict(torch.load('./chatbot1_epoch_13'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-N_PFWhIJGm",
        "outputId": "1e851f23-c506-4f6d-a4dd-221a9cb0391e"
      },
      "source": [
        "with torch.no_grad():\r\n",
        "    total_loss=0.\r\n",
        "    loss_lm = 0.\r\n",
        "    loss_cls = 0.\r\n",
        "    total_acc = 0.\r\n",
        "    for data in tqdm(train_loader,desc='step',mininterval=60):\r\n",
        "        model.eval()\r\n",
        "        data = tuple(i.to(device) for i in data)\r\n",
        "        lm_output, cls_output = Model.forward(data)        \r\n",
        "        loss1 = criterion1(lm_output.transpose(1,2),data[-1])\r\n",
        "        loss2 = criterion2(cls_output,data[-2])\r\n",
        "        loss = (loss1/config.batch_size+loss2)\r\n",
        "        loss_lm += (loss1/config.batch_size).item()\r\n",
        "        loss_cls += loss2.item()\r\n",
        "        total_loss += loss.item()\r\n",
        "        total_acc += (cls_output.argmax(-1)==data[-2]).float().mean()\r\n",
        "    print('total_loss')\r\n",
        "    print(total_loss/len(train_loader))\r\n",
        "    print('total_lm')\r\n",
        "    print(loss_lm/len(train_loader))\r\n",
        "    print('total_cls')\r\n",
        "    print(loss_cls/len(train_loader))\r\n",
        "    print('total_acc')\r\n",
        "    print(total_acc/len(train_loader))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "step:   0%|          | 0/4729 [00:00<?, ?it/s]\u001b[A\n",
            "step:  67%|██████▋   | 3172/4729 [01:00<00:29, 52.85it/s]\u001b[A\n",
            "step: 100%|██████████| 4729/4729 [01:28<00:00, 53.15it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "total_loss\n",
            "6.574096047325577\n",
            "total_lm\n",
            "6.43995144562455\n",
            "total_cls\n",
            "0.13414459955163666\n",
            "total_acc\n",
            "tensor(0.9519, device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7vI6WxEIIr1",
        "outputId": "ff27d914-a0c2-4a76-eba0-492bec304ba8"
      },
      "source": [
        "with torch.no_grad():\r\n",
        "    total_loss=0.\r\n",
        "    loss_lm = 0.\r\n",
        "    loss_cls = 0.\r\n",
        "    total_acc = 0.\r\n",
        "    for data in tqdm(val_loader,desc='step',mininterval=60):\r\n",
        "        model.eval()\r\n",
        "        data = tuple(i.to(device) for i in data)\r\n",
        "        lm_output, cls_output = Model.forward(data)        \r\n",
        "        loss1 = criterion1(lm_output.transpose(1,2),data[-1])\r\n",
        "        loss2 = criterion2(cls_output,data[-2])\r\n",
        "        loss = (loss1/config.batch_size+loss2)\r\n",
        "        loss_lm += (loss1/config.batch_size).item()\r\n",
        "        loss_cls += loss2.item()\r\n",
        "        total_loss += loss.item()\r\n",
        "        total_acc += (cls_output.argmax(-1)==data[-2]).float().mean()\r\n",
        "    print('total_loss')\r\n",
        "    print(total_loss/len(val_loader))\r\n",
        "    print('total_lm')\r\n",
        "    print(loss_lm/len(val_loader))\r\n",
        "    print('total_cls')\r\n",
        "    print(loss_cls/len(val_loader))\r\n",
        "    print('total_acc')\r\n",
        "    print(total_acc/len(val_loader))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "step: 100%|██████████| 709/709 [00:13<00:00, 52.67it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "total_loss\n",
            "19.764458034371454\n",
            "total_lm\n",
            "19.329035978895654\n",
            "total_cls\n",
            "0.4354220768067184\n",
            "total_acc\n",
            "tensor(0.8533, device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-b3vv3tIOlrq",
        "outputId": "45477951-25a5-4f5e-b9f2-1c46616f381f"
      },
      "source": [
        "with torch.no_grad():\r\n",
        "    total_loss=0.\r\n",
        "    loss_lm = 0.\r\n",
        "    loss_cls = 0.\r\n",
        "    total_acc = 0.\r\n",
        "    for data in tqdm(test_loader,desc='step',mininterval=60):\r\n",
        "        model.eval()\r\n",
        "        data = tuple(i.to(device) for i in data)\r\n",
        "        lm_output, cls_output = Model.forward(data)        \r\n",
        "        loss1 = criterion1(lm_output.transpose(1,2),data[-1])\r\n",
        "        loss2 = criterion2(cls_output,data[-2])\r\n",
        "        loss = (loss1/config.batch_size+loss2)\r\n",
        "        loss_lm += (loss1/config.batch_size).item()\r\n",
        "        loss_cls += loss2.item()\r\n",
        "        total_loss += loss.item()\r\n",
        "        total_acc += (cls_output.argmax(-1)==data[-2]).float().mean()\r\n",
        "    print('total_loss')\r\n",
        "    print(total_loss/len(test_loader))\r\n",
        "    print('total_lm')\r\n",
        "    print(loss_lm/len(test_loader))\r\n",
        "    print('total_cls')\r\n",
        "    print(loss_cls/len(test_loader))\r\n",
        "    print('total_acc')\r\n",
        "    print(total_acc/len(test_loader))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "step: 100%|██████████| 473/473 [00:08<00:00, 54.17it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "total_loss\n",
            "19.19323715372267\n",
            "total_lm\n",
            "18.747501539881586\n",
            "total_cls\n",
            "0.44573566905824846\n",
            "total_acc\n",
            "tensor(0.8573, device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sK3Tu-9JKfC"
      },
      "source": [
        "## 문장 생성과 함께 감정 상태 예측하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdUCck82LkdK"
      },
      "source": [
        "### label\r\n",
        "label이 0이면 일상, 1이면 부정/이별, 2이면 긍정/사랑\r\n",
        "허나 그렇게 정확하지만은 않다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDZROBGeJfzd",
        "outputId": "7c5f075b-c021-4c46-91b7-2d8bb820bc66"
      },
      "source": [
        "sentence_list = []\r\n",
        "label_list = []\r\n",
        "for j in test.index:\r\n",
        "    sentence=vocab.to_tokens(test.input_id[j])\r\n",
        "    label=test.label[j]\r\n",
        "    label_list.append(label)\r\n",
        "    sent = ''\r\n",
        "    for i in sentence:\r\n",
        "        if i == '<unused0>':\r\n",
        "            break\r\n",
        "        sent += i.replace('▁', ' ')\r\n",
        "    sentence_list.append(sent.replace('<usr> ',''))\r\n",
        "print(sentence_list[:10])\r\n",
        "print(label_list[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['짝남과 자연스럽게 친해질 수 있는 방법 있을까?', '말을 좀 잘 하고 싶어', '행복해져야해', '하고 싶은 게 없어', '시간이 지날수록 더 보고싶어', '학교 적을하고 잘 다닐 수 있을까?', '단체생활 힘들어', '쓸쓸하다', '이제 바람 분다', '오토바이 사고 싶어']\n",
            "[2, 0, 1, 0, 1, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqCQ-aP8MdV3",
        "outputId": "0d5922a3-9dd2-4151-9cf6-0e98fc704337"
      },
      "source": [
        "import random\r\n",
        "idx = list(range(len(sentence_list)))\r\n",
        "input = random.choice(idx)\r\n",
        "print(input)\r\n",
        "print(sentence_list[input])\r\n",
        "print(label_list[input])\r\n",
        "\r\n",
        "input_sentence = sentence_list[input]\r\n",
        "input_label = label_list[input]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500\n",
            "잊지 못하겟어\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TeHHZkvUVyj",
        "outputId": "d1e8e9f1-8e7c-47f9-fac1-2869da42fa46"
      },
      "source": [
        "input_sentence\r\n",
        "input_label"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yYcASxbN2FN",
        "outputId": "38f48da5-1e10-4846-de89-eb20f9de5ebf"
      },
      "source": [
        "toked = tokenizer(input_sentence)\r\n",
        "input_ids = torch.LongTensor([vocab['<usr>']]+vocab[toked]+[vocab['<unused0>']])\r\n",
        "print(input_ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([    2, 14037,   409, 47451, 49014, 47487,     6])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxJT6SE_TgCT",
        "outputId": "84b9aedb-2bee-4568-fa1d-4ea66717ce34"
      },
      "source": [
        "pred, lm_output = Model.inference(input_ids.to(device))\r\n",
        "print(pred)\r\n",
        "print(lm_output)\r\n",
        "result=''\r\n",
        "start = pred.index(config.cls_idx)\r\n",
        "for i in pred[start+1:]:\r\n",
        "    k = vocab.to_tokens(i)\r\n",
        "    if k == '</s>':\r\n",
        "        break\r\n",
        "    result+=k.replace('▁',' ')\r\n",
        "print(result)\r\n",
        "print(lm_output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, 14037, 409, 47451, 49014, 47487, 6, 1256, 11196, 13212, 1712, 6214, 47444, 114, 7122, 47440, 1]\n",
            "1\n",
            " 그런 나를 보면서 어떻게 잊을 수 있을까.\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}